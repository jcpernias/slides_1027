# -*- ispell-dictionary: "spanish" -*-
#+SETUPFILE: ./course-es.org

#+TITLE: {{{unit08}}} (y II)

#+PROPERTY: header-args:R :session *R-ts-intro*
#+PROPERTY: header-args:R+ :tangle yes
#+PROPERTY: header-args:R+ :exports both
#+PROPERTY: header-args:R+ :results output


#+MATS: bib
#+begin_bibbox
- Wooldridge: ::  /Introducción a la Econometría/. Capítulo 10,
  secciones 10.1, 10.2, 10.4 y 10.5.
- Stock y Watson: ::  /Introducción a la Econometría/. Capítulo 14,
  secciones 14.1 a 14.5.
#+end_bibbox

* Introducción

** Algunos procesos estocásticos

Examinaremos las principales características de algunos procesos
estocásticos importantes:

- Ruido blanco.

- Procesos autorregresivos.

- Paseo aleatorio.

- Procesos integrados.

* Ruido blanco

** Definición

El proceso estocástico $\{e_t\}$ es un *ruido blanco* si:

- $E(e_t) = 0$.

- $\var(e_t) = \sigma^2_e < \infty$.

- $\cov(e_{t+h}, e_t) = 0$, para todo $h > 0$.

** Características

- Un ruido blanco es un proceso estacionario y débilmente
  estacionario.

- Conocer los valores pasados de un ruido blanco no ayuda a predecir
  mejor $e_t$:
  #+begin_export latex
  \[
    E(e_t | e_{t-1}, e_{t-2}, \dots) = E(e_t) = 0.
  \]
  #+end_export

- Un ruido blanco no tiene *memoria*: el valor que toma en un periodo no
  tienen ninguna influencia sobre los valores futuros.

** Relación con otros conceptos

- Un ruido blanco es un proceso *i.i.d.* (independiente e
  idénticamente distribuido).

- En algunos campos coincide con el concepto de *innovaciones*.

- Un *ruido blanco gaussiano*, además de las anteriores, cumple la
  condición:
  #+begin_export latex
  \[
    e_t \sim \Normal(0, \sigma^2_e)
  \]
  #+end_export

* Procesos autorregresivos

** Procesos autorregresivos

- El valor presente de un *proceso autorregresivo* depende de los
  valores que tomó el proceso en periodos anteriores.

- Un proceso autorregresivo de orden $p$, AR($p$), cumple la siguiente
  ecuación:
  #+begin_export latex
  \[
    y_{t} = \rho_{1} y_{t-1} + \rho_{2} y_{t-2} + \dots \rho_{p} y_{t-p} + e_{t},
  \]
  #+end_export
  donde $\rho_1, \rho_2, \dots, \rho_p$ son parámetros y $\{e_t\}$ es
  un ruido blanco. El valor inicial de la secuencia es 0: $y_0 = 0$.

** Proceso AR(1)

- Nos centraremos en el proceso autorregresivo de orden 1, AR(1):
  #+begin_export latex
  \[
    y_{t} = \rho y_{t-1} +  e_{t},
  \]
  #+end_export

- La condición de estabilidad de un proceso AR(1) es:
  #+begin_export latex
  \[
    \abs{\rho} < 1
  \]
  #+end_export

- Un proceso AR estable es estacionario y débilmente dependiente.

** Esperanza de un proceso AR(1)

- $y_t$ sigue un proceso AR(1): $y_{t} = \rho y_{t-1} +  e_{t}$.

- Tomando esperanzas:
  #+begin_export latex
  \[
    E(y_{t}) = \rho E(y_{t-1}) +  E(e_{t}).
  \]
  #+end_export

- Si $y_t$ es estacionario: $E(y_t) = E(y_{t-1})œ$. Por otro
  lado, $E(e_t) = 0$. Por tanto, la expresión anterior se reduce a:
  #+begin_export latex
  \[
     E(y_{t}) = \rho E(y_{t}).
  \]
  #+end_export

- Para que la ecuación anterior sea cierta para cualquier $\rho$, se
  tiene que cumplir que $E(y_t) = 0$.

** Varianza de un proceso AR(1)

- $y_t$ sigue un proceso AR(1): $y_{t} = \rho y_{t-1} +  e_{t}$.

- La varianza de $y_t$ es:
  #+begin_export latex
  \[
     \var(y_{t}) = \rho^2 \var(y_{t-1}) + \var(e_t).
  \]
  #+end_export

- Si $y_t$ es estacionario, $\var(y_{t}) = \var(y_{t-1}) =
  \sigma^2_y$. Podemos escribir la expresión anterior como:
  #+begin_export latex
  \[
     \sigma^2_y = \rho^2 \sigma^2_y + \sigma^2_e.
  \]
  #+end_export

- Finalmente, la varianza de $y_t$ es:
  #+begin_export latex
  \[
     \sigma^2_y = \frac{\sigma^2_e}{1 - \rho^2}.
  \]
  #+end_export

** Autocovarianzas de un proceso AR(1) (I)

- Ya que $E(y_t) = 0$, la autocovarianza de $y_{t+h}$ e $y_ {t}$ es:
  #+begin_export latex
  \[
    \cov(y_{t+h}, y_t) = E(y_{t+h}  y_t)
  \]
  #+end_export

- Cuando $h = 1$:
  #+begin_export latex
  \[
    \cov(y_{t+1}, y_t) = E(y_{t+1}  y_t)
  \]
  #+end_export

- Sustituyendo $y_{t+1} = \rho y_t + e_{t+1}$:
  #+begin_export latex
  \[
    \cov(y_{t+1}, y_t) =
    E\big[(\rho y_t + e_{t+1}) y_t\big] =
    E\big[\rho y^{2}_t + y_t e_{t+1} \big]
  \]
  #+end_export

- Teniendo en cuenta que $E(y^2_t) = \sigma^2_y$ y que $E(y_t e_s) =
  0$ para $t \neq s$:
  #+begin_export latex
  \[
    \cov(y_{t+1}, y_t) = \rho \sigma^2_y.
  \]
  #+end_export

** Autocovarianzas de un proceso AR(1) (II)

- Ahora escribimos el proceso autorregresivo para $t+2$:
  #+begin_export latex
  \[
    y_{t + 2} = \rho y_{t + 1} +  e_{t + 2}.
  \]
  #+end_export

- Sustituimos $y_{t+1} = \rho y_t + e_{t+1}$:
  #+begin_export latex
  \[
    y_{t + 2} = \rho^2 y_{t} + \rho e_{t+1}+  e_{t + 2}.
  \]
  #+end_export

- La autocovarianza de $y_{t+2}$ e $y_ {t}$ es:
  #+begin_export latex
  \[
    \cov(y_{t+2}, y_t) = E(y_{t+2}  y_t) =
    E\big[(\rho^2 y_{t} + \rho e_{t+1}+  e_{t + 2}) y_t\big].
  \]
  #+end_export

- Finalmente:
  #+begin_export latex
  \[
    \cov(y_{t+2}, y_t) = \rho^2 \sigma^2_y.
  \]
  #+end_export

** Autocovarianzas de un proceso AR(1) (III)

- Mediante sustitución recursiva obtenemos:
  #+begin_export latex
  \[
    y_{t + h} =
    \rho^h y_{t} +
    \rho^{h-1} e_{t+1}  +
    \rho^{h-2} e_{t+2} + \dots +
    \rho e_{t+h-1} +
    e_{t + h}.
  \]
  #+end_export

- La autocovarianza de $y_{t+h}$ e $y_ {t}$ es:
  #+begin_export latex
  \[
    \cov(y_{t+h}, y_t) =
    E\big[(\rho^h y_{t} +
    \rho^{h-1} e_{t+1}  +
    \dots +
    e_{t + h})
    y_t\big].
  \]
  #+end_export

- Dado que $E(y^2_t) = \sigma^2_y$ y que $E(y_t e_s) = 0$ para $t \neq
  s$:
  #+begin_export latex
  \[
    \cov(y_{t+h}, y_t) = \rho^h \sigma^2_y.
  \]
  #+end_export

** Autocorrelaciones de un proceso AR(1)

- Autocorrelación entre $y_{t+h}$ e $y_ {t}$:
  #+begin_export latex
  \[
    \corr(y_{t+h}, y_{t}) = \frac{\cov(y_{t+h}, y_{t})}{
      \sqrt{\var(y_{t+h})\var(y_{t})}
    }
  \]
  #+end_export

- Teniendo en cuenta que
  #+begin_export latex
  \begin{gather*}
    \var(y_{t+h}) = \var(y_{t}) = \sigma^2_y, \\
    \intertext{y que}
    \cov(y_{t+h}, y_{t}) = \rho^h \sigma^2_y, \\
    \intertext{se obtiene:}
    \corr(y_{t+h}, y_{t}) = \rho^h.
  \end{gather*}
  #+end_export


** Estacionariedad y dependencia débil

- Si $\abs{\rho} < 1$ el proceso AR(1) es estacionario:

  + $E(y_t) = 0$.

  + $\var(y_t) = \sigma^2_e / (1 - \rho^2)$.

  + $\cov(y_{t+h}, y_{t}) = \rho^h \sigma^2_y$, para $h > 0$.

- Si $\abs{\rho} < 1$ el proceso AR(1) es débilmente dependiente,
  puesto que $\corr(y_{t+h}, y_t) = \rho^h$ converge a 0 conforme $h
  \to \infty$ y la convergencia es geométrica.


* Paseo aleatorio

** Definición

- Un *paseo aleatorio* es un proceso estocástico que se puede expresar
  como:
  #+begin_export latex
  \[
     y_t = y_{t-1} + e_t,
  \]
  #+end_export
  donde $e_t$ es un ruido blanco.

- Sustituyendo recursivamente se obtiene:
  #+begin_export latex
  \[
    y_t = e_t + e_{t-1} + \dots + e_1 + y_0,
  \]
  #+end_export
  donde $y_0$ es el valor inicial del proceso. Supondremos que $y_0 =
  0$, por lo que:
  #+begin_export latex
  \[
    y_t = e_t + e_{t-1} + \dots + e_1.
  \]
  #+end_export

** Esperanza y varianza

- La esperanza de un paseo aleatorio es 0:
  #+begin_export latex
  \[
    E(y_t) =  E(e_t) +  E(e_{t-1}) + \dots +  E(e_1) = 0.
  \]
  #+end_export


- La varianza de un paseo aleatorio es creciente en el tiempo:
  #+begin_export latex
  \[
    \var(y_t) =  \var(e_t) +  \var(e_{t-1}) + \dots +  \var(e_1) = t \sigma^2_e.
  \]
  #+end_export

** Autocovarianzas y autocorrelaciones

- La autocovarianza entre $y_{t+h}$ e $y_t$ es:
  #+begin_export latex
  \[
    \cov(y_{t+h}, y_t) = E(y_{t+h} y_t) =
    E\big[( y_{t} + e_{t+1}  + \dots + e_{t + h}) y_t\big]  = t \sigma^2_e.
  \]
  #+end_export

- La autocorrelación entre $y_{t+h}$ e $y_t$ es:
  #+begin_export latex
  \[
    \corr(y_{t+h}, y_t) = \sqrt{\frac{t}{t+h}}
  \]
  #+end_export


** No estacionariedad

Un paseo aleatorio no cumple las condiciones de la estacionariedad
débil:

- $E(y_t) = 0$.

- $\var(y_t) = t \sigma^2_e$.

- $\cov(y_{t+h}, y_{t}) = t \sigma^2_e$, para $h > 0$.


** Dependencia fuerte

- Las autocorrelaciones de un paseo aleatorio son:
  #+begin_export latex
  \[
    \corr(y_{t+h}, y_t) = \sqrt{\frac{t}{t+h}}
  \]
  #+end_export

- $\corr(y_{t+h}, y_t)$ converge a 0 conforme $h \to \infty$, pero la
  convergencia es muy lenta.

** Paseo aleatorio con deriva

- Un *paseo aleatorio con deriva* se puede expresar como:
  #+begin_export latex
  \[
     y_t = \alpha_0 + y_{t-1} + e_t,
  \]
  #+end_export
  donde $e_t$ es un ruido blanco y el parámetro $\alpha_0$ es el
  *término de deriva*.

- Sustituyendo recursivamente y teniendo en cuenta que $y_0 = 0$:
  #+begin_export latex
  \[
    y_t = \alpha_0 t + e_t + e_{t-1} + \dots + e_1.
  \]
  #+end_export

- El valor esperado de $y$ es función de $t$:
  #+begin_export latex
  \[
     E(y_t) = \alpha_0 t
  \]
  #+end_export


* Procesos integrados

** La diferencia de un paseo aleatorio

- Un paseo aleatorio puede escribirse como:
  #+begin_export latex
  \[
    y_t = y_{t-1} + e_t
  \]
  #+end_export

- Restando $y_{t-1}$ de ambos lados:
  #+begin_export latex
  \[
    y_t - y_{t-1} = e_t
  \]
  #+end_export

- La diferencia de un paseo aleatorio es un ruido blanco:
  #+begin_export latex
  \[
     \incr y_t = e_t
  \]
  #+end_export

** Procesos integrados

- El proceso $y_t$ es *integrado de orden 1*, I(1), si $y_t$ es un
  proceso altamente persistente, pero su diferencia, $\incr y_t$, es
  débilmente dependiente.

- Si es necesario diferenciar dos veces para obtener un proceso
  débilmente dependiente, el proceso $y_t$ es *integrado de orden 2*,
  I(2).

- Un proceso débilmente dependiente es *integrado de orden 0*, I(0),
  puesto que no es necesario aplicar ninguna diferencia.
