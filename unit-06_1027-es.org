# -*- ispell-dictionary: "spanish" -*-
#+SETUPFILE: ./course-es.org

#+TITLE: {{{unit06}}}

#+MATS: bib
#+begin_bibbox
- Wooldridge: ::  /Introducción a la Econometría/. Capítulos 7.5 y 8.5.
#+end_bibbox

* Introducción


** Eventos cuantitativos

¿Es posible usar el modelo de regresión para analizar los
determinantes de *eventos cuantitativos*?


** Variable dependiente binaria

La variable dependiente sólo toma dos valores:

- $y_i = 1$, en aquellas observaciones donde ocurre el evento que
  estamos estudiando; y

- $y_i = 0$, en las observaciones en que no ocurre el evento.


** Modelo de regresión múltiple

- Modelo de regresión:
  #+begin_export latex
  \[
    y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + u.
  \]
  #+end_export

- ¿Cuál es la interpretación de los parámetros cuando $y$ es una
  variable binaria?

** Probabilidad condicional

- Si $y$ es una variable dicotómica la esperanza condicional de $y$
  coincide con la probabilidad condicional de que $y = 1$:
  #+begin_export latex
  \[
    \Exp(y | \bm{x}) = \Pr(y = 1 | \bm{x}).
  \]
  #+end_export

- *Modelo de probabilidad lineal* (MLP): la probabilidad de que $y = 1$
  es lineal en los parámetros $\beta_0, \beta_1, \dots, \beta_k$:
  #+begin_export latex
  \[
    \Pr(y = 1 | \bm{x}) = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k.
  \]
  #+end_export


** Interpretación de los parámetros

- Podemos expresar el MLP como:
  #+begin_export latex
  \[
    p(\bm{x}) = \beta_0 + \beta_1 x_1  + \dots + \beta_k x_k,
  \]
  #+end_export
  donde $p(\bm{x})$ es una forma abreviada de escribir $\Pr(y = 1 |
  \bm{x})$.

- La pendiente $\beta_j$ mide cómo cambia la probabilidad de que $y =
  1$ cuando cambia $x_j$, manteniendo inalteradas el resto de
  explicativas:
  #+begin_export latex
  \[
    \beta_j = \frac{\Delta p(\bm{x})}{\Delta x_j}.
  \]
  #+end_export



** Ventajas y limitaciones

- En comparación a otros modelos más sofisticados, el MLP:

  + es más fácil de usar e interpretar;

  + proporciona resultados similares cuando las variables explicativas
    toman valores cercanos a sus medias.

- El MLP puede generar probabilidades fuera del intervalo $[0, 1]$,
  por lo que no es adecuado para algunas aplicaciones.



* Estimación por MCO


** Estimación

- Las estimaciones de los parámetros del MLP se pueden obtener por MCO.

- Los residuos se obtienen de la forma usual:
  #+begin_export latex
  \[
    \uhat_i = y_i - \bhat_0 - \bhat_1 x_{1i} - \dots - \bhat_k x_{ki}.
  \]
  #+end_export

** Predicción

- Las predicciones de MCO se pueden interpretar como estimaciones de
  la probabilidad de $y = 1$:
  #+begin_export latex
  \[
    \phat_i = \bhat_0 + \bhat_1 x_{1i} + \dots + \bhat_k x_{ki}
  \]
  #+end_export

- Para predecir la variable dependiente se suele usar la regla:
  #+begin_export latex
  \[
    \ytilde_{i} =
    \begin{cases}
      1 & \text{si $\phat_{i} \geq 0.5$}, \\
      0 & \text{si $\phat_{i} < 0.5$}.
    \end{cases}
  \]
  #+end_export


** Bondad del ajuste

- En los modelos con variable dependiente binaria se suele usar como
  medida de bondad del ajuste el *porcentaje de observaciones
  correctamente predichas*

- Para calcular esta medida tabulamos las predicciones $\yhat$ y los
  valores de $y$:

  #+begin_export latex
  \begin{center}
    \begin{tabular}{*{3}{>{$}c<{$}}}
      \toprule
      & \ytilde_i = 0 & \ytilde_i = 1 \\
      \midrule
      y_i = 0 & n_{00} & n_{01} \\
      y_i = 1 & n_{10} & n_{11} \\
      \bottomrule
    \end{tabular}
    \vspace*{1ex}
  \end{center}
  #+end_export

  Sumando $n_{00} + n_{11}$ se obtiene el número de observaciones
  correctamente predichas.


** Insesgadez y consistencia

- Los supuestos *RLM.1* a *RLM.4* no impiden que la variable
  dependiente $y$ sea binaria.

- Si se cumplen los supuestos *RLM.1* a *RLM.4* el estimador MCO es
  insesgado y consistente.


** Heteroscedasticidad

- En el modelo lineal de probabilidad se incumple el supuesto *RLM.5*.

- La varianza condicional del término de error puede escribirse como:
  #+begin_export latex
  \[
    \var(u|\bm{x}) = p(\bm{x})\big(1 - p(\bm{x})\big).
  \]
  #+end_export

** Uso de MCO

- El modelo lineal de probabilidad puede estimarse por MCO.

- Para contrastar hipótesis sobre los parámetros es necesario usar
  estimaciones robustas a heteroscedasticidad de $\var(\bhat)$.


* Estimación eficiente

** Varianza del término de error

- La varianza condicional del término de error del MLP puede
  escribirse como:
  #+begin_export latex
  \[
    \var(u_i|\bm{x_i}) = p(\bm{x_i})\big(1 - p(\bm{x_i})\big).
  \]
  #+end_export

- La probabilidad $p_i = p(\bm{x_i})$ depende de parámetros
  desconocidos.

- Se podría aplicar MCPF si se reemplaza $p_i$ por una estimación,
  $\phat_i$.

** Mínimos Cuadrados Ponderados Factibles

- Si se cumplen los supuestos *RLM.1* a *RLM.4*, MCO es un estimador
  insesgado.

- Con las estimaciones $\bhat_0, \bhat_1, \dots, \bhat_k$ se estima la
  probabilidad de que $y = 1$:
  #+begin_export latex
  \[
    \phat_i = \bhat_0 + \bhat_1 x_{1i} + \dots + \bhat_k x_{ki}
  \]
  #+end_export

- Los pesos para obtener el estimador MCPF son $w_i = 1 / \hhat_i$, donde:
  #+begin_export latex
  \[
    \hhat_i = \phat_i (1 - \phat_i).
  \]
  #+end_export

** Limitaciones

- El estimador MCPF no puede obtenerse si para algunas observaciones
  se obtiene $\phat_i < 0$ o $\phat_i > 0$.

- Aunque se han propuesto alternativas, en esos casos es mejor usar
  MCO y contrastes de hipótesis robustos a heteroscedasticidad.
