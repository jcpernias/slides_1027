# -*- ispell-dictionary: "spanish" -*-
#+SETUPFILE: ./course-es.org

#+TITLE: {{{unit06}}}

#+MATS: bib
#+begin_bibbox
- Wooldridge: ::  /Introducción a la Econometría/. Capítulos 7.5 y 8.5.
#+end_bibbox

* Introducción


** Eventos cuantitativos

¿Es posible usar el modelo de regresión para analizar los
determinantes de *eventos cuantitativos*?


** Variable dependiente binaria

La variable dependiente sólo toma dos valores:

- $y_i = 1$, cuando se produce el evento que estamos estudiando;

- $y_i = 0$, cuando no se produce.


** Modelo de regresión múltiple

- Modelo de regresión:
  #+begin_export latex
  \[
    y = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k + u.
  \]
  #+end_export

- ¿Cuál es la interpretación de los parámetros cuando $y$ es una
  variable binaria?


** Probabilidad condicional

- Si $y$ es una variable binaria, su esperanza condicional coincide
  con la probabilidad condicional de que $y = 1$:
  #+begin_export latex
  \[
    \Exp(y | \bm{x}) = \Pr(y = 1 | \bm{x}).
  \]
  #+end_export

- *Modelo de probabilidad lineal* (MLP): la probabilidad de que $y$
  tome el valor $1$ es una función lineal en los parámetros $\beta_0,
  \beta_1, \dots, \beta_k$:
  #+begin_export latex
  \[
    \Pr(y = 1 | \bm{x}) = \beta_0 + \beta_1 x_1 + \dots + \beta_k x_k.
  \]
  #+end_export


** Interpretación de los parámetros

- Podemos expresar el MLP como:
  #+begin_export latex
  \[
    p(\bm{x}) = \beta_0 + \beta_1 x_1  + \dots + \beta_k x_k,
  \]
  #+end_export
  donde $p(\bm{x})$ es una forma abreviada de escribir $\Pr(y = 1 |
  \bm{x})$.

- La pendiente $\beta_j$ mide cómo cambia la probabilidad de que $y =
  1$ cuando cambia $x_j$, manteniendo inalteradas el resto de
  explicativas:
  #+begin_export latex
  \[
    \beta_j = \frac{\Delta p(\bm{x})}{\Delta x_j}.
  \]
  #+end_export



** Ventajas y limitaciones

- En comparación a otros modelos más sofisticados, el MLP:

  + es más fácil de usar e interpretar;

  + proporciona resultados similares cuando las variables explicativas
    toman valores cercanos a sus medias.

- El MLP puede generar probabilidades fuera del intervalo $(0, 1)$,
  por lo que no es adecuado para algunas aplicaciones.



* Estimación por MCO


** Estimación

- Los parámetros del MLP se pueden estimar por MCO.

- Los residuos se obtienen de la forma usual:
  #+begin_export latex
  \[
    \uhat_i = y_i - \bhat_0 - \bhat_1 x_{1i} - \dots - \bhat_k x_{ki}.
  \]
  #+end_export


** Predicción

- Las predicciones de MCO se pueden interpretar como estimaciones de
  la probabilidad de $y = 1$:
  #+begin_export latex
  \[
    \phat_i = \bhat_0 + \bhat_1 x_{1i} + \dots + \bhat_k x_{ki}
  \]
  #+end_export

- Para predecir la variable dependiente se suele usar la regla:
  #+begin_export latex
  \[
    \ytilde_{i} =
    \begin{cases}
      1 & \text{si $\phat_{i} \geq 0.5$}, \\
      0 & \text{si $\phat_{i} < 0.5$}.
    \end{cases}
  \]
  #+end_export


** Bondad del ajuste

Tabulación cruzada de $y$ e $\ytilde$:
#+begin_export latex
\begin{center}
  \begin{tabular}{*{3}{>{$}c<{$}}}
    \toprule
    & \ytilde_i = 0 & \ytilde_i = 1 \\
    \midrule
    y_i = 0 & n_{00} & n_{01} \\
    y_i = 1 & n_{10} & n_{11} \\
    \bottomrule
  \end{tabular}
  \vspace*{1ex}
\end{center}
#+end_export

- Número de predicciones correctas: $n_{00} + n_{11}$.

- Número total de observaciones: $n = n_{00} + n_{01} + n_{10} +
  n_{11}$.

- *Porcentaje de predicciones correctas*: $100 (n_{00} + n_{11}) /
  n$. Se usa para medir la bondad del ajuste de modelos con
  variable dependiente binaria.

** Insesgadez y consistencia

- Los supuestos *RLM.1* a *RLM.4* no son incompatibles con que la
  variable dependiente sea binaria.

- Si se cumplen los supuestos *RLM.1* a *RLM.4* el estimador MCO es
  insesgado y consistente.


** Heteroscedasticidad

- En el modelo lineal de probabilidad se incumple el supuesto *RLM.5*.

- La varianza condicional del término de error puede escribirse como:
  #+begin_export latex
  \[
    \var(u|\bm{x}) = p(\bm{x})\big(1 - p(\bm{x})\big).
  \]
  #+end_export


** Uso de MCO

- Los parámetros del modelo lineal de probabilidad pueden estimarse
  por MCO.

- Para contrastar hipótesis o construir intervalos de confianza es
  necesario usar estimaciones robustas a heteroscedasticidad de
  $\var(\bhat)$.


* Estimación eficiente


** Varianza del término de error

- La varianza condicional del término de error del MLP puede
  escribirse como:
  #+begin_export latex
  \[
    \var(u_i|\bm{x_i}) = p(\bm{x_i})\big(1 - p(\bm{x_i})\big).
  \]
  #+end_export

- La probabilidad $p_i = p(\bm{x_i})$ depende de parámetros
  desconocidos.

- Se podría aplicar MCPF si se reemplaza $p_i$ por una estimación,
  $\phat_i$.


** Mínimos Cuadrados Ponderados Factibles

- Si se cumplen los supuestos *RLM.1* a *RLM.4*, MCO es un estimador
  insesgado.

- Con las estimaciones $\bhat_0, \bhat_1, \dots, \bhat_k$ se estima la
  probabilidad de que $y = 1$:
  #+begin_export latex
  \[
    \phat_i = \bhat_0 + \bhat_1 x_{1i} + \dots + \bhat_k x_{ki}
  \]
  #+end_export

- Los pesos para obtener el estimador MCPF son $w_i = 1 / \hhat_i$, donde:
  #+begin_export latex
  \[
    \hhat_i = \phat_i (1 - \phat_i).
  \]
  #+end_export


** Limitaciones

- El estimador MCPF no puede obtenerse si para algunas observaciones
  se obtiene $\phat_i \leq 0$ o $\phat_i \geq 1$.

- Aunque se han propuesto alternativas, en esos casos es mejor usar
  MCO y contrastes de hipótesis robustos a heteroscedasticidad.
