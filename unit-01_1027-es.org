#+SETUPFILE: ./course-es.org

#+TITLE: {{{unit01}}}

#+MATS: bib
#+begin_bibbox
- Wooldridge: ::  /Introducción a la Econometría/. Capítulos 3, 4, 6 y 7.
#+end_bibbox

#+PROPERTY: header-args:R :session *R* :exports results :results output :eval yes
#+PROPERTY: header-args:R :var orgbackend=(prin1-to-string org-export-current-backend)
#+MACRO: Rtable (eval (concat "#+header: :results output " (prin1-to-string org-export-current-backend)))

* El modelo clásico de regresión lineal


** Supuestos

- *RLM.1*: Linealidad en los parámetros.

- *RLM.2*: Muestreo aleatorio.

- *RLM.3*: Ausencia de multicolinealidad perfecta.

- *RLM.4*: Media condicional nula.

- *RLM.5*: Homoscedasticidad.

- *RLM.6*: Normalidad.


** RLM.1: Linealidad en los parámetros

El *modelo poblacional* puede expresarse como:
#+begin_export latex
\[
  y = \beta_{0} + \beta_{1} x_{1} + \dots + \beta_{k} x_{k} + u
\]
#+end_export
donde:
- $y$ es la *variable dependiente*,

- $\beta_0, \beta_1, \dots, \beta_k$ son los *parámetros*
  desconocidos,

- $x_1, x_2, \dots, x_k$ son las *variables explicativas* y

- $u$ es un *término de error* no observable.


** RLM.2: Muestreo aleatorio

Disponemos de una *muestra aleatoria* de $n$ observaciones:
#+begin_export latex
\[
  \{(x_{1i}, x_{2i}, \dots, x_{ki}, y_{i}); i = 1, 2, \dots, n \}
\]
#+end_export


** RLM.3: No hay colinealidad perfecta

En la muestra se cumplen *todas* las condiciones:

- El número de observaciones, $n$, es mayor que el de parámetros, $k +
  1$.

- Ninguna de las variables explicativas es constante.

- No existen *relaciones lineales exactas* entre las explicativas.


** RLM.4: Media condicional nula

El valor esperado del término de error para cualquier combinación de
valores que tomen las variables explicativas es 0:
#+begin_export latex
\[
  \Exp(u | x_{1}, x_{2}, \dots, x_{k}) = 0
\]
#+end_export


** RLM.5: Homoscedasticidad

La varianza del término de error no depende de los valores
que tomen las explicativas:
#+begin_export latex
\[
  \var(u | x_{1}, x_{2}, \dots, x_{k}) = \sigma^{2}
\]
#+end_export


** RLM.6: Normalidad

El término de error es *independiente* de las variables explicativas y
su distribución es *normal* con media 0 y varianza $\sigma^2$:
#+begin_export latex
\[
  u \sim \Normal(0, \sigma^{2})
\]
#+end_export


* Estimación


** Función de regresión muestral

*Función de regresión muestral*:
#+begin_export latex
\[
  \yhat = \bhat_{0} + \bhat_{1} x_{1} + \dots + \bhat_{k} x_{k}
\]
#+end_export
donde:
- $\yhat$ son los *predicciones*,

- $\bhat_0, \bhat_1, \dots, \bhat_k$ son las *estimaciones* de los parámetros.


** Residuos

*Residuos*:
#+begin_export latex
\[
   \uhat = y - \yhat
\]
#+end_export

La función de regresión muestral también puede expresarse como:
#+begin_export latex
\[
  y = \bhat_{0} + \bhat_{1} x_{1} + \dots + \bhat_{k} x_{k} + \uhat
\]
#+end_export


** Estimación por MCO

El estimador de *mínimos cuadrados ordinarios*, MCO, minimiza la suma
del cuadrado de los residuos:
#+begin_export latex
\[
  \min_{\{\bhat_{0}, \dots, \bhat_{k}\}} \sum_{i = 1}^n \uhat_i^{2}
\]
#+end_export


** Ecuaciones normales

El estimador MCO se obtiene resolviendo las *ecuaciones normales*:
#+begin_export latex
\begin{gather*}
  \sum_{i = 1}^n \uhat_{i} = 0 \\
  \sum_{i = 1}^n \uhat_{i} x_{ji} = 0 \quad (j = 1, 2, \dots, k)
\end{gather*}
#+end_export


** Sumas de cuadrados

Suma de cuadrados total:
#+begin_export latex
\[
   \SCT = \sum_{1=1}^n (y_i - \ymean)^{2}
\]
#+end_export

Suma de cuadrados explicada:
#+begin_export latex
\[
   \SCE = \sum_{1=1}^n (\yhat_i - \ymean)^{2}
\]
#+end_export

Suma de cuadrados de los residuos:
#+begin_export latex
\[
  \SCR = \sum_{1=1}^n \uhat_i^{2}
\]
#+end_export

La estimación por MCO garantiza que:
#+begin_export latex
\[
  \SCT = \SCE + \SCR
\]
#+end_export


** Bondad del ajuste

- Error típico de la regresión:
  #+begin_export latex
  \[
    \SER = \sqrt{\frac{\SCR}{n - k - 1}}
  \]
  #+end_export

- Coeficiente de determinación:
  #+begin_export latex
  \[
    \Rsq = 1 - \frac{\SCR}{\SCT}
  \]
  #+end_export

- $\Rsq$ ajustado:
  #+begin_export latex
  \[
    \Rbarsq = 1 - \frac{\SCR/(n - k - 1)}{\SCT/(n - 1)}
  \]
  #+end_export


* Propiedades del estimador de MCO


** Propiedades de muestras pequeñas

- Se refieren al método de estimación, MCO, no a las estimaciones
  obtenidas con una muestra particular.

- Dependen del cumplimiento de los supuestos del modelo de regresión
  lineal clásico.

- No dependen del tamaño muestral: son válidas para cualquier $n$.



** Insesgadez

*Insesgadez*: El valor esperado de las estimaciones coincide con los
parámetros poblacionales:
#+begin_export latex
\[
  \Exp(\bhat_{j})  = \beta_{j} \quad (j = 0, 1, \dots, k).
\]
#+end_export

El estimador de MCO es insesgado si se cumplen los supuestos *RLM.1* a
*RLM.4*.


** Omisión de variables relevantes

- Ejemplo: modelo que cumple los supuestos *RLM.1* a *RLM.4*:
  #+begin_export latex
  \[
    y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + u
  \]
  #+end_export

- Realizamos la regresión de $y$ sobre $x_1$ omitiendo la variable
  $x_2$.

- ¿Qué ocurre si especificamos un modelo donde falta una de las
  explicativas?


** Sesgo

- Cuando se omite $x_2$:
  #+begin_export latex
  \[
    \Exp(\bhat_{1}) = \beta_{1} + \beta_2 \delta_{21}
  \]
  #+end_export
  donde $\delta_{21}$ es la pendiente de la regresión de $x_2$ sobre
  $x_1$.

- *Sesgo* por omisión de $x_2$:
  #+begin_export latex
  \[
    \operatorname{Sesgo}(\bhat_{1}) = \Exp(\bhat_{1}) - \beta_{1} = \beta_2 \delta_{12}
  \]
  #+end_export


** Sesgo de variable omitida

El estimador de MCO presenta sesgo cuando se cumplen *las dos*
condiciones:

- $\beta_2 \neq 0$: las variables omitidas son *relevantes*.

- $\delta_{21} \neq 0$: las variables omitidas están correlacionadas
  con alguna de las variables incluidas en la regresión.


** Varianza del estimador MCO

Si se cumplen los *supuestos de Gauss-Markov* (*RLM.1* a *RLM.5*):
#+begin_export latex
\[
  \var(\bhat_{j} | \bm{x}) = \frac{\sigma^{2}}{\SCT_{j}(1 - \Rsq_{j})}
  \quad (j = 1, 2, \dots, k)
\]
#+end_export
donde:
- $\SCT_j = \sum_{i=1}^n(x_{ij} - \bar{x}_j)^2$

- $\Rsq_j$ es el coeficiente de determinación de una regresión de
  $x_j$ sobre el resto de explicativas.


** Eficiencia

*Teorema de Gauss-Markov*: Si se cumplen los supuestos de
Gauss-Markov, el estimador MCO es el *estimador lineal insesgado
óptimo*.

- Un *estimador lineal* es una función lineal de los valores de la
  variable dependiente:
  #+begin_export latex
  \[
     \bhat_j = \sum_{i=1}^n w_{ij} y_i
  \]
  #+end_export

- Dentro de un grupo de estimadores, el *estimador óptimo* es el que
  tiene menor varianza muestral.


** Distribución muestral

Bajo los supuestos del modelo clásico de regresión lineal (*RLM.1* a *RLM.6*):
#+begin_export latex
\[
  \bhat_{j} | \bm{x} \sim \Normal\big(\beta_j, \var(\bhat_j | \bm{x})\big)
  \quad (j = 0, 1, \dots, k)
\]
#+end_export


* Contraste de hipótesis sobre un parámetro


** Hipótesis nula e hipótesis alternativa

- *Hipótesis nula*, $H_0$: la hipótesis para la que queremos
  determinar si hay o no suficiente evidencia en contra.

- *Hipótesis alternativa*, $H_1$: la hipótesis que aceptaremos si
  rechazamos la hipótesis nula.


** Contrastes acerca de un parámetro

- Modelo poblacional:
  #+begin_export latex
  \[
    y = \beta_{0} + \beta_{1} x_{1} + \dots + \beta_{j} x_{j} + \dots +
    \beta_{k} x_{k} + u.
  \]
  #+end_export

- Hipótesis nula: el valor del parámetro $\beta_j$ es igual a $b_j$,
  #+begin_export latex
  \[
    H_{0}\!: \beta_{j} = b_{j}.
  \]
  #+end_export


** Estadístico de contraste

El *estadístico $\bm{t}$* para la $H_0\!: \beta_j = b_j$ es:
#+begin_export latex
\[
  t = \frac{\bhat_{j} - b_{j}}{\se(\bhat_{j})},
\]
#+end_export
donde $\se(\bhat_{j})$ es el error típico del estimador del parámetro
$\beta_j$.


** Distribución bajo la hipótesis nula

#+MATS: figcol file:fig-02_1027-tdens-*.pdf 0.5

Si se cumplen los supuestos del modelo clásico de regresión lineal,
*cuando la hipótesis nula es cierta* el estadístico $t$ se distribuye
como una variable aleatoria $t$ con $n - k - 1$ grados de libertad:
#+begin_export latex
\[
  t \sim t_{n-k-1}.
\]
#+end_export


** La hipótesis alternativa

La hipótesis alternativa es, usualmente, la negación de la hipótesis
nula:
#+begin_export latex
\[
  H_{1}\!: \beta_{j} \neq b_{j}.
\]
#+end_export


** La región de rechazo

#+MATS: figcol file:fig-02_1027-ttest-rejection-area-*.pdf 0.5

La hipótesis alternativa determina la *región de rechazo*: valores
grandes en valor absoluto del estadístico $t$ son evidencia en contra
de la hipótesis nula.


** Nivel de significación

- El *nivel de significación* determina el tamaño de la región de
  rechazo.

- Es la probabilidad de rechazar la hipótesis nula cuando ésta es
  cierta:
  #+begin_export latex
  \[
    \alpha = \Pr(\text{rechazar $H_{0}$}\,|\,\text{$H_{0}$ es cierta}).
  \]
  #+end_export


** Valores críticos

- Los *valores críticos* delimitan la región de rechazo.

- Se determinan a partir de la distribución bajo la hipótesis nula y
  del nivel de significación.


** Valores críticos de un contraste de dos colas


#+MATS: figcol file:fig-02_1027-ttest-two-tails-*.pdf 0.5

En un contraste de $H_0\!: \beta_j = b_j$ frente a $H_1\!: \beta_j
\neq b_j$, el valor crítico $c_{\alpha}$ cumple:
#+begin_export latex
\[
  \Pr(|t_{n - k - 1}| > c_{\alpha/2}) = \alpha.
\]
#+end_export


** Regla de decisión

#+MATS: figcol file:fig-02_1027-ttest-h0-rejected-*.pdf 0.5

*Regla de decisión*: se rechaza $H_0$ (en favor de $H_1$) a un nivel
de significación $\alpha$ cuando el estadístico $t$ cae en la región
de rechazo:
#+begin_export latex
\[
  |t| > c_{\alpha/2}.
\]
#+end_export


** No rechazo

#+MATS: figcol file:fig-02_1027-ttest-h0-not-rejected-*.pdf 0.5

Decimos que *no se rechaza* la $H_0$ cuando el estadístico $t$ se
sitúa fuera de la región de rechazo. Evitamos expresiones como
"/aceptar la hipótesis nula/".


** Valor-$p$

El *valor-$\bm{p}$* es la probabilidad, cuando $H_0$ es cierta, de
observar un valor del estadístico de contraste al menos tan grande
como el que se ha obtenido.


** Valor-$p$ de un contraste de dos colas

#+MATS: figcol file:fig-02_1027-ttest-p-value-*.pdf 0.5

El valor-$p$ se corresponde con el área de las dos colas delimitadas
por el estadístico de contraste:
#+begin_export latex
\[
  \text{valor-}p = \Pr(|t_{n-k-1}| > |t|).
\]
#+end_export


** Regla de decisión con el valor-$p$

*Regla de decisión*: se rechaza $H_0$ a un nivel de significación
$\alpha$ cuando el valor-$p$ es menor que el nivel de significación
$\alpha$:
#+begin_export latex
\[
  \text{valor-}p < \alpha.
\]
#+end_export


** Interpretación del valor-$p$

- El valor-$p$ *no* es la probabilidad de que $H_0$ sea cierta.

- Mide la intensidad de la evidencia en contra de $H_0$:
  cuanto más pequeño es el valor-$p$, menos soportan los datos la
  $H_0$.


** Resumen

- Contraste de $H_0\!: \beta_j = b_j$ frente a $H_{1}\!: \beta_{j}
  \neq b_{j}$.

- Estadístico de contraste:
  #+begin_export latex
  \[
    t = \frac{\bhat_{j} - b_{j}}{\se(\bhat_{j})}.
  \]
  #+end_export

- Se rechaza $H_0$ a un nivel de significación $\alpha$ si:

  + $|t| > c_{\alpha/2}$, donde $c_{\alpha/2}$ se obtiene de la
    distribución $t_{n - k - 1}$; o

  + $\text{valor-}p < \alpha$, donde el valor-$p$ se obtiene de la
    distribución $t_{n - k - 1}$.


** Contrastes de significación individual

- Es un caso especial del contraste $t$ de dos colas: $H_0\!: \beta_j = 0$ frente a $H_{1}\!: \beta_{j} \neq 0$.

- El estadístico de contraste es el *cociente $\bm{t}$*:
  #+begin_export latex
  \[
    t = \frac{\bhat_{j}}{\se(\bhat_{j})}.
  \]
  #+end_export

- Cuando se rechaza la $H_0$ se suele usar expresiones como "$x_j$ es
  *estadísticamente significativa* al nivel $\alpha$".


** Contrastes de una cola (I)

#+MATS: figcol file:fig-02_1027-ttest-one-tail-pos-*.pdf 0.5

- $H_0\!: \beta_j = b_j$ frente a $H_{1}\!: \beta_{j}
  > b_{j}$.

- Estadístico de contraste:
  #+begin_export latex
  \[
    t = (\bhat_{j} - b_{j})/\se(\bhat_{j}).
  \]
  #+end_export

- Se rechaza $H_0$ si $t > c_{\alpha}$, donde:
  #+begin_export latex
  \[
    \Pr(t_{n-k-1} > c_{\alpha}) = \alpha.
  \]
  #+end_export


** Contrastes de una cola (y II)

#+MATS: figcol file:fig-02_1027-ttest-one-tail-neg-*.pdf 0.5

- $H_0\!: \beta_j = b_j$ frente a $H_{1}\!: \beta_{j}
  < b_{j}$.

- Estadístico de contraste:
  #+begin_export latex
  \[
    t = (\bhat_{j} - b_{j})/\se(\bhat_{j}).
  \]
  #+end_export

- Se rechaza $H_0$ si $t < -c_{\alpha}$, donde:
  #+begin_export latex
  \[
    \Pr(t_{n-k-1} < -c_{\alpha}) = \alpha.
  \]
  #+end_export


** Contrastes de una cola y valores-$p$

- Los valores-$p$ que presentan por defecto los programas estadísticos
  son válidos para contrastes de dos colas.

- Si el estadístico $t$ está en la misma cola de la región de rechazo,
  el valor-$p$ será la mitad del que imprime el programa informático.

- Si el estadístico $t$ está en la cola opuesta a la de la región de
  rechazo, el valor-$p$ será mayor que $0.5$ y no se rechazará la
  $H_0$ a ninguno de los niveles de significación habituales.


* Intervalos de confianza


** Intervalos de confianza

Un *intervalo de confianza* para el parámetro $\beta_j$ a un *nivel de
confianza* de $1-\alpha$ es un intervalo $(\bar{\beta}_j,
\ubar{\beta}_j)$ que contendrá el valor del parámetro $\beta_j$ con
una probabilidad $1-\alpha$:
#+begin_export latex
\[
  \Pr(\ubar{\beta}_j < \beta_j < \bar{\beta}_j) = 1 - \alpha.
\]
#+end_export



** Estimación por intervalos

*Estimación por intervalos*: en vez de obtener un único valor
(*estimación puntual*), calculamos un intervalo de valores para el
parámetro $\beta_j$.

#+MATS: fig file:fig-02_1027-conf-interval-*.pdf



** Contraste de hipótesis

*Contraste de hipótesis*: Rechazamos $H_0\!: \beta_j = b_j$ frente a
$H_1\!: \beta_j \neq b_j$ a un nivel de significación $\alpha$ si
$b_j$ no está incluido en el intervalo de confianza $(\bar{\beta}_j,
\ubar{\beta}_j)$ construido a un nivel de confianza $1 - \alpha$.

#+MATS: fig file:fig-02_1027-conf-interval-test-*.pdf



** Cálculo

Si se cumplen los supuestos del modelo clásico de regresión lineal,
*RLM.1* a *RLM.6*, los límites de un intervalo a un nivel de confianza
$1 - \alpha$ para el parámetro $\beta_j$ son:
#+begin_export latex
\[
  \bhat_j \pm c_{\alpha/2} \cdot \se(\bhat_j),
\]
#+end_export
donde $c_{\alpha/2}$ es el valor crítico de dos colas de una
distribución $t_{n-k-1}$ a un nivel de significación $\alpha$.


** Simetría


El intervalo de confianza del parámetro $\beta_j$ es simétrico y su
centro es la estimación $\bhat_j$.

#+MATS: fig file:fig-02_1027-conf-interval-estimation-*.pdf


* Contrastes de hipótesis lineales sobre los parámetros


** Hipótesis lineales sobre los parámetros

- La hipótesis nula se compone de una o más *combinaciones lineales*
  de los parámetros. Por ejemplo:

  + $H_0\!: \beta_3 = 0, \beta_4 = 1$.

  + $H_0\!: \beta_3  + \beta_4 = 1, \beta_5 = -1$.

- La hipótesis alternativa es la negación de la nula: al menos una de
  las igualdades no se cumple.


** Restricciones de exclusión

- *Restricciones de exclusión múltiple*: las pendientes de $q$
  regresores son nulas:
  #+begin_export latex
  \[
    H_{0}\!: \beta_{1} = \beta_{2} = \dots = \beta_{q} = 0.
  \]
  #+end_export
- En la hipótesis alternativa al menos una de las pendientes $\beta_{1}, \beta_{2}, \dots, \beta_{q}$ es distinta de $0$:
  #+begin_export latex
  \[
    H_{1}\!: \text{no $H_0$}.
  \]
  #+end_export


** Modelo no restringido

*Modelo no restringido*: los parámetros pueden tomar cualquier valor y
es válido tanto bajo la $H_0$ como bajo la $H_1$:
#+begin_export latex
\[
  y = \beta_{0} + \beta_{1} x_{1} + \dots + \beta_{q} x_{q} +
  \beta_{q+1} x_{q+1} + \dots +  \beta_{k} x_{k} + u.
\]
#+end_export


** Modelo restringido

*Modelo restringido*: los parámetros $\beta_1, \dots, \beta_q$ son
iguales a $0$ y sólo es válido bajo la $H_0$:
#+begin_export latex
\[
  y = \beta_{0} + \beta_{q+1} x_{q+1} + \dots +  \beta_{k} x_{k} + u.
\]
#+end_export



** Bondad del ajuste

- Si las restricciones de exclusión son ciertas, no debe haber mucha
  diferencia entre el ajuste del modelo restringido y el del modelo no
  restringido.

- El contraste se basará en la comparación de las sumas de cuadrados
  de los residuos del modelo restringido, $\SCRr$, y del modelo no
  restringido, $\SCRnr$.


** Estadístico de contraste

El estadístico de contraste es el *estadístico $\bm{F}$*:
#+begin_export latex
\[
  F = \frac{(\SCRr - \SCRnr) / q}{\SCRnr / (n - k - 1)}.
\]
#+end_export


** Distribución bajo la hipótesis nula

Si se cumplen los supuestos del modelo clásico de regresión lineal,
*cuando la hipótesis nula es cierta* el estadístico $F$ se distribuye
como una variable aleatoria $F$ con $q$ y $n - k - 1$ grados de libertad:
#+begin_export latex
\[
  F \sim F_{q, n-k-1}.
\]
#+end_export


** Región de rechazo

#+MATS: figcol file:fig-02_1027-ftest-rejection-area-*.pdf 0.5

- Valor crítico a un nivel de significación $\alpha$:
  #+begin_export latex
  \[
    \Pr(F_{q, n-k-1} > c_{\alpha}) = \alpha.
  \]
  #+end_export

- Valores del estadístico $F$ mayores que $c_{\alpha}$ representan
  evidencia en contra de la hipótesis nula.



** La regla de decisión

*Regla de decisión*: se rechaza $H_0$ a un nivel de significación
$\alpha$ cuando:

- El estadístico $F$ cae en la región de rechazo:
  #+begin_export latex
  \[
    F > c_{\alpha}.
  \]
  #+end_export

- El valor-$p$ es menor que el nivel de significación $\alpha$. El
  valor-$p$ es:
  #+begin_export latex
  \[
    \text{valor-}p = \Pr(F_{q, n-k-1} > F).
  \]
  #+end_export



** El contraste $F$ y el $R^2$

/Para contrastar restricciones de exclusión/ el estadístico $F$ puede
calcularse a partir de los coeficientes de determinación del modelo
restringido, $\Rsqr$, y del modelo no restringido, $\Rsqnr$:
#+begin_export latex
\[
  F = \frac{(\Rsqnr - \Rsqr) / q}{(1 - \Rsqnr) / (n - k - 1)}.
\]
#+end_export


** Significación de la regresión

- *Significación conjunta de la regresión*: contraste de la hipótesis
  nula de que todas las pendientes son nulas:
  #+begin_export latex
  \[
    H_0\!: \beta_1 = \beta_2 = \dots = \beta_k = 0.
  \]
  #+end_export

- El estadístico $F$ en términos del coeficiente de determinación es:
  #+begin_export latex
  \[
    F = \frac{\Rsq / k}{(1 - \Rsq) / (n - k - 1)}.
  \]
  #+end_export



** Restricciones lineales generales

- Se pueden usar los estadísticos $F$ basados en las sumas de
  cuadrados de los residuos para contrastar restricciones lineales
  generales.

- Las fórmulas basadas en los coeficientes de determinación *sólo*
  pueden usarse si los modelos restringido y no restringido tienen la
  *misma variable dependiente*.

- Cualquier conjunto de restricciones lineales siempre puede
  transformarse en restricciones de exclusión mediante una
  reparametrización del modelo.


* COMMENT Predicción


** Predicción puntual

- Predicción del valor medio.

- Predicción del valor.


** Intervalos de predicción

- Valor medio.

- Valor de la variable dependiente.



# Local Variables:
# ispell-local-dictionary: "spanish"
# End:
