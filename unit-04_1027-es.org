# -*- ispell-dictionary: "spanish" -*-
#+SETUPFILE: ./course-es.org

#+TITLE: {{{unit04}}}

#+MATS: bib
#+begin_bibbox
- Wooldridge: ::  /Introducción a la Econometría/. Capítulo 8.
#+end_bibbox

* Introducción


** Homoscedasticidad

*Homoscedasticidad*: la dispersión alrededor de la función de
regresión poblacional, FRP, es constante. Todas las observaciones son
igual de informativas a la hora de determinar por donde pasa la FRP.


** Heteroscedasticidad

*Heteroscedasticidad*: la dispersión alrededor de la función de
regresión poblacional, FRP, cambia con los valores de la
explicativa. Algunas observaciones contienen menos información para
estimar la FRP.


** Varianza condicional

*Heteroscedasticidad*: La varianza condicional del término de error no
es constante:
#+begin_export latex
\[
  \Exp(u_i^2|x_{1i}, x_{2i}, \dots, x_{ki}) = \sigma^2_i
\]
#+end_export
y, en general, $\sigma^2_i \neq \sigma^2_j$, cuando $i \neq j$.


* Consecuencias de la heteroscedasticidad


** Insesgadez y consistencia

- La heteroscedasticidad no afecta al supuesto **RLM.4**.

- Si se cumplen los supuestos **RLM.1** a **RLM.4**, el estimador MCO
  es insesgado y consistente aunque exista heteroscedasticidad.



** Varianza de los estimadores de MCO

Cuando hay heteroscedasticidad, las fórmulas usuales de las varianzas
de los estimadores de MCO, $\var(\bhat)$, no son válidas incluso en
muestras grandes.


** Contraste de hipótesis

Cuando el término de error es heteroscedástico, los procedimientos
habituales de contraste de hipótesis dejan de ser válidos incluso
en muestras grandes.


** Eficiencia

- Con heteroscedasticidad, MCO no es el estimador lineal insesgado
  óptimo.

- Si se conoce el patrón de la heteroscedasticidad es posible
  construir estimadores más eficientes que MCO.


* Inferencia robusta a heteroscedasticidad


** Errores típicos robustos

Un procedimiento común con datos de corte transversal:

- Estimar los parámetros por MCO.

- Modificar el cálculo de los errores típicos de los estimadores para
  que sean válidos (asintóticamente) haya o no haya
  heteroscedasticidad.


** Varianza del estimador MCO

- Modelo de regresión simple:
  #+begin_export latex
  \[
    y_i = \beta_0 + \beta_1 x_i + u_i.
  \]
  #+end_export

- Se cumplen los supuestos *RLM.1* a *RLM.4*.

- Heteroscedasticidad:
  #+begin_export latex
  \[
    \var(u_{i} | x_{1i}) = \sigma^{2}_{i}.
  \]
  #+end_export

- La varianza del estimador MCO es:
  #+begin_export latex
  \[
    \var(\bhat_{1}) = \frac{\sum(x_{i} - \bar{x})^{2}\sigma^2_{i}}{%
      \Big[\sum(x_{i} - \bar{x})^{2}\Big]^{2}}.
  \]
  #+end_export


** Estimación robusta de $\var(\bhat)$

- [[https://en.wikipedia.org/wiki/Halbert_White][Halbert White]] mostró que se puede obtener una estimación consistente
  de $\var(\bhat)$ exista o no heteroscedasticidad:
  #+begin_export latex
  \[
    \widehat{\var}_R(\bhat_{1}) = \frac{\sum(x_{i} - \bar{x})^{2} \uhat^2_i}{%
      \Big[\sum(x_{i} - \bar{x})^{2}\Big]^{2}}.
  \]
  #+end_export

- Posteriormente, se han descrito alternativas que son asintóticamente
  equivalentes a la propuesta original de White pero, en ciertas
  situaciones, son superiores en muestras pequeñas.


** Contrastes $t$ robustos

- Se obtiene un estadístico válido asintóticamente para contrastar la
  hipótesis nula $\beta_j = b$ utilizando la estimación de MCO,
  $\bhat_j$, y el error típico robusto a heteroscedasticidad,
  $\se_R(\bhat_j)$:
  #+begin_export latex
  \[
    t_{j} = \frac{\bhat_{j} - b}{\se_{R}(\bhat_{j})}.
  \]
  #+end_export


** Contrastes de hipótesis lineales

- En presencia de heteroscedasticidad no es válido el estadístico $F$
  que compara las $\SCR$ o los $\Rsq$ de los modelos restringido y no
  restringido. Tampoco existe una versión robusta de este contraste.

- Para contrastar restricciones lineales generales sobre los
  parámetros del modelo, utilizamos contrastes de Wald construidos con
  estimaciones de $\var(\hat{\bm\beta})$ robustas a
  heteroscedasticidad.


* Contrastes de heteroscedasticidad


** ¿Por qué?


- Si no hay grandes problemas de heteroscedasticidad no sería necesario
  usar errores típicos robustos.

- MCO es el estimador óptimo (en cierto sentido) cuando el término de
  error es homoscedástico. Si hay heteroscedasticidad quizá queramos
  emplear estimadores más eficientes que MCO.

** Contrastes de especificación

- Tratamos de verificar si se cumple alguno de los supuestos sobre el
  modelo poblacional.

- La hipótesis nula suele ser que la especificación es correcta.

- A menudo se utiliza un contraste de multiplicadores de Lagrange.

** Contrastes de heteroscedasticidad

- La hipótesis nula es el supuesto *RLM.5*:
  #+begin_export latex
  \[
    H_0\!: \var(u_i | x_{1i}, \dots, x_{ki}) = \sigma^2.
  \]
  #+end_export

- Bajo la hipótesis alternativa hay heteroscedasticidad:
  #+begin_export latex
  \[
    H_1\!: \var(u_i | x_{1i}, \dots, x_{ki}) = \sigma^2_i.
  \]
  #+end_export

** Enfoque general

- Podemos reescribir la hipótesis nula como:
  #+begin_export latex
  \[
    H_0\!: \Exp(u^2_i | x_{1i}, \dots, x_{ki}) = \sigma^2.
  \]
  #+end_export

- Si no se cumple $H_0$ la esperanza condicional de $u^2$ depende de
  los regresores. Si la relación fuera lineal:
  #+begin_export latex
  \[
     u^2  = \delta_0   + \delta_1 x_{1} +  \dots + \delta_k x_{k} + \text{error}.
  \]
  #+end_export

- Si observáramos $u_i$ podríamos contrastar homoscedasticidad
  mediante un contraste de:
  #+begin_export latex
  \[
    H_0\!: \delta_1 = \delta_2 = \dots = \delta_k = 0
  \]
  #+end_export


** Regresión auxiliar

- Los contrastes modernos de heteroscedasticidad utilizan una
  regresión auxiliar cuya variable dependiente es $\uhat^2$.

- Cada contraste se diferencia por los $k_{aux}$ regresores que se
  incluyen.

- El estadístico $\LM = n \Rsq_{aux}$ se distribuye bajo la $H_0$ como
  una $\chi^2_{k_{aux}}$.

- También puede usarse el contraste de significación de la regresión:
  #+begin_export latex
  \[
    F = \frac{\Rsq_{aux} / k_{aux}}{(1 - \Rsq_{aux}) / (n - k_{aux} - 1)}.
  \]
  #+end_export


** Contraste de Breusch-Pagan

- En la actualidad, se usa comúnmente la variante, debida a [[https://en.wikipedia.org/wiki/Roger_Koenker][Koenker]],
  del contraste que propusieron [[https://en.wikipedia.org/wiki/Trevor_S._Breusch][Breusch]] y [[https://en.wikipedia.org/wiki/Adrian_Pagan][Pagan]].

- En la regresión auxiliar se usan como regresores las $k$ variables
  explicativas del modelo de regresión original.


** Contraste de Breusch-Pagan: ejemplo

- Modelo de regresión con 3 explicativas:
  #+begin_export latex
  \[
    y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + u_i.
  \]
  #+end_export

- Después de estimar por MCO se obtienen los residuos $\uhat$.

- Regresión auxiliar:
  #+begin_export latex
  \[
    \uhat^2_i = \delta_0 + \delta_1 x_{1i} + \delta_2 x_{2i} + \delta_3 x_{3i} + \text{error}.
  \]
  #+end_export

- Se estima por MCO la regresión auxiliar y se calcula el estadístico
  $\LM$ o el estadístico $F$ de la regresión auxiliar.


** Contraste de White: motivación

[[https://en.wikipedia.org/wiki/Halbert_White][White]] concluye que la heteroscedasticidad sólo es problemática si la varianza condicional de $u$ depende de:

- las variables explicativas: $x_1, x_2, \dots, x_k$;

- sus cuadrados: $x^2_1, x^2_2, \dots, x^2_k$; o

- sus productos cruzados: $x_1 \cdot x_2, x_1 \cdot x_3, \dots$

** Contraste de White

- La regresión auxiliar incluye  las explicativas, sus cuadrados y sus productos cruzados.

- Pueden aparecer problemas de multicolinealidad perfecta en la
  regresión auxiliar, especialmente si hay variables ficticias. En ese caso, habría que eliminar de la regresión auxiliar los regresores redundantes.


** Contraste de White: ejemplo

- Modelo de regresión con 3 explicativas:
  #+begin_export latex
  \[
    y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + u_i.
  \]
  #+end_export

- Después de estimar por MCO se obtienen los residuos $\uhat$.

- Regresión auxiliar:
  #+begin_export latex
  \begin{align*}
    \uhat^2_i
    = \delta_0 &+ \delta_1 x_{1i} + \delta_2 x_{2i} + \delta_3 x_{3i}
    + \delta_4 x^{2}_{1i} + \delta_{5} x^{2}_{2i} + \delta_6 x^{2}_{3i} \\
    &+ \delta_7 x_{1i} x_{2i} + \delta_8 x_{1i} x_{3i} + \delta_9 x_{2i}x_{3i}
     +  \text{error}.
  \end{align*}
  #+end_export

- Se estima por MCO la regresión auxiliar y se calcula el estadístico
  $\LM$ o el estadístico $F$ de la regresión auxiliar.


** Contraste de White: alternativas (I)

- En la regresión auxiliar del contraste de White hay un gran número
  de parámetros, lo que puede traducirse en baja **potencia**
  (capacidad de detectar heteroscedasticidad cuando realmente está
  presente).

- Para mitigar ese problema, en ocasiones se omiten los productos
  cruzados en la regresión auxiliar:
  #+begin_export latex
  \begin{equation*}
    \uhat^2_i
    = \delta_0 + \delta_1 x_{1i} + \delta_2 x_{2i} + \delta_3 x_{3i}
    + \delta_4 x^{2}_{1i} + \delta_{5} x^{2}_{2i} + \delta_6 x^{2}_{3i}
      +  \text{error}.
  \end{equation*}
  #+end_export

** Contraste de White: alternativas (y II)

Wooldridge propone una variante del contraste de White donde sólo se
incluyen en la regresión auxiliar las predicciones de MCO,
$\yhat$, y sus cuadrados, $\yhat^2$:
#+begin_export latex
\begin{equation*}
  \uhat^2_i = \delta_0 + \delta_1 \yhat_{i} + \delta_2 \yhat^2_{i} +  \text{error}.
\end{equation*}
#+end_export




* Estimación eficiente


** Mínimos cuadrados generalizados (MCG)

La idea detrás de MCG es:

1. Transformar el modelo de forma que se cumplan los supuestos de
   Gauss-Markov.

2. Aplicar MCO al modelo transformado.


** Mínimos cuadrados ponderados (I)

- Función de regresión poblacional:
  #+begin_export latex
  \[
    y = \beta_0 + \beta_1 x_{1} + \beta_2 x_{2} + u
  \]
  #+end_export

- Consideremos el caso en que:
  #+begin_export latex
  \[
    V(u | x_{1}, x_{2}) = \Exp(u^2 | x_{1}, x_{2}) = \sigma^2 x^2_{2}
  \]
  #+end_export


** Mínimos cuadrados ponderados (II)

- Transformación del modelo: dividimos por $x_{2}$:
  #+begin_export latex
  \[
    \frac{y}{x_{2}} = \beta_0 \frac{1}{x_{2}}+ \beta_1 \frac{x_{1}}{x_{2}} + \beta_2 \frac{x_{2}}{x_{2}} + \frac{u}{x_{2}}
  \]
  #+end_export

- Reescribimos el modelo como:
  #+begin_export latex
  \[
    y^* = \beta_2 + \beta_0 x^*_{0} + \beta_1 x^*_{1} + u^*
  \]
  #+end_export
  donde $y^* = y / x_{2}$, $x^*_{0} = 1 / x_{2}$, $x^*_{1} = x_{1} / x_{2}$, $u^* = u / x_{2}$.


** Mínimos cuadrados ponderados (III)

- El término de error del modelo transformado es homoscedástico:
  #+begin_export latex
  \begin{align*}
    \var(u^*| x_{1}, x_{2})
    &= \Exp((u/x_{2})^2|x_{1}, x_{2}) \\
    &= (1/x_{2})^2\Exp(u^2| x_{1}, x_{2}) \\
    &= (1/x^2_{2}) \sigma^2 x_{2}^2 \\
    &= \sigma^2
  \end{align*}
  #+end_export

- El estimador ELIO consiste en aplicar MCO al modelo transformado.


** Mínimos cuadrados ponderados (IV)

Resumen:

- La varianza del término de error es proporcional a $h_i$ que es una
  función conocida de variables observables:
  #+begin_export latex
  \[
    \var(u | x_{1}, x_{2}, \dots, x_{k}) = \sigma^2 h
  \]
  #+end_export

- Transformamos el modelo dividiendo todas las variables (y el término
  constante) por $\sqrt{h}$.

- Estimamos el modelo transformado por MCO.


** MCP factibles (I)

- Hasta ahora hemos supuesto que conocemos $h$.

- ¿Qué podemos hacer si no observamos $h$?


** MCP factibles (II)

- Función de regresión poblacional:
  #+begin_export latex
  \[
    y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i
  \]
  #+end_export

- Consideremos ahora el caso en que:
  #+begin_export latex
  \[
    \Exp(u_i^2 | x_{1i}, x_{2i}) = \sigma^2 \exp(\delta_0 + \delta_1 x_{2i} + \delta_2 x^2_{2i})
  \]
  #+end_export
  La función de dispersión depende de parámetros desconocidos:
  #+begin_export latex
  \[
    h_i =\exp(\delta_0 + \delta_1 x_{2i} + \delta_2 x^2_{2i})
  \]
  #+end_export


** MCP factibles (III)

- Para aplicar el principio de MCG primero debemos estimar los
  parámetros de los que depende $h_i$ y obtener una estimación de la
  función de dispersión, $\hat{h}_i$.

- Transformamos el modelo dividiendo por $\sqrt{\hat{h}_i}$.

- Aplicamos MCO al modelo transformado.


** Estimación de $h_i$ (I)

- Varianza condicional:
  #+begin_export latex
  \[
    \Exp(u_i^2 | x_{1i}, x_{2i}) = \sigma^2 \exp(\delta_0 + \delta_1 x_{2i} + \delta_2 x^2_{2i})
  \]
  #+end_export

- A partir de la ecuación anterior, podemos escribir:
  #+begin_export latex
  \[
    u_i^2 = \sigma^2 \exp(\delta_0 + \delta_1 x_{2i} + \delta_2 x^2_{2i}) v_i
  \]
  #+end_export

- Tomando logaritmos:
  #+begin_export latex
  \[
    \log(u_i^2) = \theta_0 + \delta_1 x_{2i} + \delta_2 x^2_{2i} + e_i
  \]
  #+end_export


** Estimación de $h_i$ (II)

- Estimamos los parámetros de:
  #+begin_export latex
  \[
    \log(u_i^2) = \theta_0 + \delta_1 x_{2i} + \delta_2 x^2_{2i} + e_i
  \]
  #+end_export
  reemplazando $u_i$ por los residuos de mínimos cuadrados,
  $\hat{u}_i$

- Finalmente obtenemos $\hat{h}_i$ tomando la exponencial de los
  valores predichos en la regresión anterior.


** Comparación con MCO

- MCP y MCO son insesgados bajo heteroscedasticidad. No debería haber
  una gran diferencia entre ambas estimaciones.

- No son comparables los coeficientes de determinación y el error
  típico de la regresión de MCO y MCP.

- Igual que con MCO, es posible usar matrices de covarianzas robustas
  después de estimar por MCP.


** Alternativas a MCP

- La heteroscedasticidad suele estar asociada con el "tamaño" de las
  observaciones.

- Con frecuencia, los problemas de heteroscedasticidad pueden
  mitigarse usando logaritmos.
