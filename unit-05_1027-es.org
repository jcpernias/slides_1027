# -*- ispell-dictionary: "spanish" -*-
#+SETUPFILE: ./course-es.org

#+TITLE: {{{unit04}}} (y II)

#+MATS: bib
#+begin_bibbox
- Wooldridge: ::  /Introducción a la Econometría/. Capítulo 8.
#+end_bibbox


* Ineficiencia de MCO


** Heteroscedasticidad

#+MATS: figcol img/het3.pdf 0.5

Con *heteroscedasticidad* la dispersión alrededor de la FRP
cambia con los valores de la explicativa.


** Ineficiencia de MCO

#+MATS: figcol img/het4.pdf 0.5

La *ineficiencia de MCO* se debe a que se tratan por igual todas las
observaciones, aunque no todas contienen información igual de precisa
para estimar la FRP.


** Estimación eficiente

*Estimación eficiente* con heteroscedasticidad:

- Se asigna un peso diferente a cada observación.

- Las observaciones más imprecisas reciben ponderaciones menores.


** Mínimos Cuadrados Generalizados

El estimador eficiente cuando existe heteroscedasticidad pertenece a
la familia de *Mínimos Cuadrados Generalizados* (MCG). Los estimadores
MCG consisten en:

1. *Transformar el modelo* de forma que se cumplan los supuestos de
   Gauss-Markov.

2. Estimar el *modelo transformado por MCO*.


* Mínimos cuadrados ponderados


** Modelo de regresión

El modelo de regresión cumple los supuestos *RLM.1* a *RLM.4*:
#+begin_export latex
\[
  y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i}
  + \dots + \beta_k x_{ki} + u_i.
\]
#+end_export


** Heteroscedasticidad

- La varianza del término de error puede expresarse como:
  #+begin_export latex
  \[
    \var(u_i | x_{1i}, x_{2i}, \dots, x_{ki}) =
    \sigma^2 h(x_{1i}, x_{2i}, \dots, x_{ki}) =
    \sigma^2 h(\bm{x}_i).
  \]
  #+end_export

- La función $h(\bm{x}_i)$ toma siempre valores positivos y expresa la
  relación entre las explicativas y la varianza del término de error.

- La constante desconocida $\sigma^2$ es positiva.


** Heteroscedasticidad conocida

- Supondremos que la función $h(\bm{x}_i)$ *es conocida* y no depende
  de parámetros desconocidos.


** Transformación del modelo

- Si conocemos $h_i = h(\bm{x}_i)$ podemos dividir el modelo original
  por $\sqrt{h_i}$.

- El término de error del modelo transformado, $u^*_i = u_i /
  \sqrt{h_i}$, es homoscedástico:
  #+begin_export latex
  \begin{align*}
    \var(u_i^*| \bm{x}_i)
    &= \Exp\big((u_i/\sqrt{h_i})^2| \bm{x}_i\big) \\
    &= (1/h_{i})\Exp(u^2| \bm{x}_i) \\
    &= (1/h_i) \sigma^2 h_i \\
    &= \sigma^2.
  \end{align*}
  #+end_export


** Ejemplo

- Modelo original. Si $x_{2i}$ siempre toma valores positivos y
  $h(x_{1i}, x_{2i}) = x_{2i}$:
  #+begin_export latex
  \begin{gather*}
    y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + u_i, \\
    \var(u_i | x_{1i}, x_{2i}) = \sigma^2 x_{2i}.
  \end{gather*}
  #+end_export

- Modelo transformado:
  #+begin_export latex
  \begin{gather*}
    y^*_i = \beta_0 x^*_{0i} + \beta_1 x^*_{1i} + \beta_2 x^*_{2i} + u^*_i, \\
    \var(u^*_i | x_{1i}, x_{2i}) = \sigma^2.
  \end{gather*}
  #+end_export
  donde
  #+begin_export latex
  \begin{equation*}
    y^*_i = y_i / \sqrt{x_{2i}},\quad
    x^*_{0i} = 1 / \sqrt{x_{2i}},\quad
    x^*_{1i} = x_{1i} / \sqrt{x_{2i}},\quad
    x^*_{2i} = x_{2i} / \sqrt{x_{2i}}.
  \end{equation*}
  #+end_export


** Mínimos Cuadrados Ponderados

Estimador de *Mínimos Cuadrados Ponderados*, (MCP):

- La varianza del término de error es proporcional a $h_i$ que es una
  función conocida de variables observables:
  #+begin_export latex
  \[
    \var(u_i | x_{1i}, x_{2i}, \dots, x_{ki}) = \sigma^2 h_i.
  \]
  #+end_export

- Se transforma el modelo dividiendo por $\sqrt{h_i}$.

- Se estima el modelo transformado por MCO.


** Propiedades de MCP

- Si se cumplen los supuestos *RLM.1* a *RLM.4* el estimador MCP es
  insesgado y consistente.

- El modelo transformado cumple con los supuestos de Gauss-Markov, por
  lo que MCP es el estimador lineal insesgado óptimo.


** Predicciones y residuos

- El modelo transformado sólo sirve para obtener las estimaciones de
  MCP de los parámetros: $\btilde_0, \btilde_1, \dots, \btilde_k$.

- Para calcular las predicciones usamos las variables originales sin
  transformar:
  #+begin_export latex
  \[
    \ytilde_i = \btilde_{0}  + \btilde_{1} x_{1i} + \dots + \btilde_{k} x_{ki}.
  \]
  #+end_export

- También usamos las variables originales para calcular los residuos:
  #+begin_export latex
  \[
    \utilde_i = y_i - \ytilde_i = y_i - \btilde_{0}  - \btilde_{1} x_{1i} - \dots - \btilde_{k} x_{ki}.
  \]
  #+end_export



** Suma ponderada de los cuadrados de los residuos

- El estimador MCP minimiza la suma *ponderada de residuos al cuadrado*:
  #+begin_export latex
  \[
    \sum_{i} \utilde^{2}_{i}/h_{i} =
    \sum_{i} \big(
    y_{i} - \btilde_{0} - \btilde_{1} x_{1i} - \dots - \btilde_{k} x_{ki}
    \big)^{2}/h_{i}.
  \]
  #+end_export

- El *peso* o *ponderación*, $w_i$, que se asigna a cada observación
  es la inversa de $h_i$, $w_i = 1 / h_i$.


** Bondad del ajuste

En general, no es posible comparar el $R^2$ obtenido con MCP y el
obtenido con MCO:

- Diferentes programas informáticos calculan de forma diferente el
  $R^2$ de MCP y los distintos métodos no son equivalentes.

- Los $R^2$ que se calculan para MCP no se pueden interpretar como
  medidas de bondad del ajuste.


* Mínimos cuadrados ponderados factibles


** Heteroscedasticidad desconocida

- Hasta ahora hemos supuesto que conocemos $h_i$. ¿Qué podemos hacer
  en caso contrario?

- Aunque no conozcamos la forma exacta de la heteroscedasticidad, con
  frecuencia sabemos que está relacionada con algunas de las variables
  del modelo.

- El estimador de *Mínimos Cuadrados Ponderados Factibles* (MCPF) se
  obtiene de forma análoga al estimador de MCP salvo que utiliza una
  estimación de $h_i$ para obtener el modelo transformado.


** Modelización de la heteroscedasticidad

Especificaremos un modelo para la varianza del término de error que
dependa de variables observables y de parámetros desconocidos. Por
ejemplo, podemos escribir:
#+begin_export latex
\begin{gather*}
  \var(u_i | \bm{x}_{i}) = \sigma^2 h(\bm{x}_{i}) \\
  \intertext{con}
  h(\bm{x}_{i}) = \exp(\delta_{0} + \delta_{1} x_{1i} + \dots + \delta_{k} x_{ki}),
\end{gather*}
#+end_export
donde $\delta_0, \delta_1, \dots, \delta_k$ son parámetros
desconocidos y la función exponencial garantiza que $h(\bm{x}_{i}) >
0$.


** Estimación de $h_i$ (I)

Varianza condicional del término de error:
#+begin_export latex
\[
  \Exp(u_i^2 | \bm{x}_{i}) = \sigma^2
  \exp(\delta_{0} + \delta_{1} x_{1i} + \dots + \delta_{k} x_{ki}).
\]
#+end_export
Si observásemos el término de error, $u_i$, podríamos estimar:
#+begin_export latex
\[
  \log(u_i^2) =
  \alpha_0 + \delta_{1} x_{1i} + \dots +
  \delta_{k} x_{ki} + \text{error}.
\]
#+end_export
Para poder estimar los parámetros de la regresión anterior,
reemplazamos $u_i$ por los residuos de MCO, $\hat{u}_i$.


** Estimación de $h_i$ (y II)

1. Estimamos por MCO los parámetros de la función de regresión:
  #+begin_export latex
  \[
    \log(\uhat_i^2) = \alpha_0 + \delta_{1} x_{1i} +
    \dots + \delta_{k} x_{ki} + \text{error}.
  \]
  #+end_export

2. Obtenemos $\hhat_i$ a partir de las predicciones de la regresión
   anterior:
   #+begin_export latex
   \[
     \hhat_i = \exp(\ahat_0 + \dhat_{1} x_{1i} + \dots + \dhat_{k} x_{ki}).
   \]
   #+end_export


** Mínimos Cuadrados Ponderados Factibles

Procedimiento para la estimación por *Mínimos Cuadrados Ponderados
Factibles* (MCPF):

#+ATTR_LATEX: :options [label=\textbf{Paso \arabic*}:, wide=0pt, leftmargin=1em]
1. Se estima el modelo por MCO y se guardan los residuos $\uhat_i$.

2. Se estima por MCO una regresión de $\log(\uhat^2_i)$ sobre las
   explicativas.

3. Se obtiene $\hhat_i$ tomando la exponencial de las predicciones de
   la regresión del paso anterior.

4. Se estima el modelo por MCP usando $1/\hhat_i$ como ponderaciones.


** Propiedades de MCPF

- Al usar una estimación de $h_i$, MCPF no es insesgado ni lineal.

- Si se cumplen los supuestos *RLM.1* a *RLM.4* el estimador de MCPF es consistente.

- Si la especificación de la heteroscedasticidad es correcta, MCPF es
  asintóticamente más eficiente que MCO.


** Especificaciones alternativas

Se han usado especificaciones alternativas de la
heteroscedasticidad. En consecuencia se modificaría el *Paso 2* del
procedimiento para obtener el estimador MCPF descrito antes:

- Regresar $\log(\uhat^2_i)$ sobre un subconjunto de las explicativas.

- Regresar $\log(\uhat^2_i)$ sobre las explicativas, sus cuadrados y
  productos cruzados.

- Regresar $\log(\uhat^2_i)$ sobre las predicciones, $\yhat$, y sus cuadrados, $\yhat^2_i$.

Los restantes pasos del procedimiento de estimación no se alterarían.


* Otras cuestiones


** Comparación con MCO

- MCO y MCP(F) son estimadores consistentes cuando se cumplen los
  supuestos *RLM.1* a *RLM.4*.

- Grandes diferencias en las estimaciones de MCO y de MCP(F)
  indicarían el posible incumplimiento de otras de las hipótesis de
  Gauss-Markov.


** Inferencia robusta

- Si el supuesto sobre la naturaleza de la heteroscedasticidad es
  correcto, los contrastes $t$ y $F$ calculados a partir de las
  estimaciones MCP(F) tienen validez asintótica.

- Si el supuesto no captura toda la heteroscedasticidad, sería
  necesario usar un estimador robusto de $\var(\btilde)$ después de la
  estimación MCP(F).

- En general, es recomendable el uso de métodos de inferencia robustos
  a heteroscedasticidad con los estimadores de MCO y de MCP(F).
